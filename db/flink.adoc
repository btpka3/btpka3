


## 参考
* link:https://flink.apache.org/[apache flink]
** Application Development
/ link:https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/overview/[Table API & SQL]/
*** SQL / Queries / link:https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/sql/queries/overview/[Overview]
** Functions / link:https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/functions/udfs/#table-functions[User-defined Functions]
** Testing link:https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/testing/[Testing]

* link:https://blog.csdn.net/qq_41982570/article/details/123780773[流计算框架 Flink 与 Storm 的性能对比]
* link:https://zhuanlan.zhihu.com/p/159036199[Apache 流框架“三剑客” Flink、Spark Streaming、Storm对比分析（一）]
* 阿里云 link:https://www.aliyun.com/product/bigdata/sc[实时计算Flink版]
* link: https://github.com/aliyun/aliyun-log-flink-connectors[aliyun/aliyun-log-flink-connector]


apache flink : 窗口函数支持较为完善， 支持 Exactly Once 消息投递方式，
支持 流处理、批处理。
Flink 可以运行在 YARN 上，与 HDFS 协同工作。

由 java,scala 混合编程实现、核心是 java 编写。

初期的Spark Streaming是通过将数据流转成批(micro-batches)，
即收集一段时间(time-window)内到达的所有数据，并在其上进行常规批处，
所以严格意义上，还不能算作流式处理。
但是Spark从版本开始推出基于 Continuous Processing Mode的 Structured Streaming，
支持按事件时间处理和端到端的一致性，但是在功能上还有一些缺陷，比如对端到端的exactly-once语义的支持。

弹性分布式数据集 RDD(Resilient Distributed Dattsets)

DataStream API :
提供了机遇事件时间、处理事件的流式处理模型。
适用于是实时数据处理和流式计算。
具有低延迟，高吞吐的特性，也支持时间处理、窗口操作、状态管理、容错机制。

Table API
适用于离线处理、批处理、使用关系型数据。提供类似SQL的查询语言，可以对数据进行查询、过滤、聚合等操作。


## 单机启动

[source,shell]
----
./bin/start-cluster.sh
# 如果是默认配置，可浏览器访问 : http://localhost:8081  (如果无法访问，请check是否端口被占用)
ps aux | grep flink
./bin/stop-cluster.sh

# 提交一个简单任务
./bin/flink run examples/streaming/WordCount.jar
tail log/flink-*-taskexecutor-*.out
----

## sql client

### 启动

[source,shell]
----
./bin/sql-client.sh
----

### session configuration property

[source,sql]
----
./bin/sql-client.sh
----


### module

[source,sql]
----
-- 列出已加载的 module
SHOW MODULES;
SHOW FULL MODULES;

-- 加载、卸载 moudle
LOAD MODULE hive WITH ('hive-version' = '2.3.4');
UNLOAD MODULE hive;

-- 调整 moudle 的顺序
USE MODULES hive, core ;

select 111;
select NOW();
select CURRENT_ROW_TIMESTAMP();
select MAP['k1','v1','k2','v2'];
select ARRAY['v1', 'v2'];

select k1 FROM MAP( 'k1','v1','k2','v2');

SET 'sql-client.execution.result-mode' = 'tableau';
SET 'execution.runtime-mode' = 'batch';

SELECT
  name,
  COUNT(*) AS cnt
FROM
  (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name)
GROUP BY name;

SELECT
  name,
  MAP( 'name',name)
FROM
  (VALUES ('Bob'), ('Alice'), ('Greg'), ('Bob')) AS NameTable(name)
GROUP BY name;

----

### jar

[source,sql]
----
SHOW JARS;
ADD    JAR '/path/to/xxx.jar' ;
REMOVE JAR '/path/to/xxx.jar' ;
----

### catalog

[source,sql]
----
-- 列出所有的 catalog
SHOW CATALOGS;
-- 获取当前的 catalog
SHOW CURRENT CATALOG;
-- 切换当前的 catalog
USE CATALOG catalog1;
----


### database

[source,sql]
----
SHOW DATABASES;
SHOW CURRENT DATABASE;
-- 切换当前的 database
USE database1;
USE catalog1.database1;
CREATE DATABASE mydb WITH (...);
----

### table

[source,sql]
----
-- 列出所有表
SHOW TABLES;
-- 创建一个新表
CREATE TABLE my_table (...) WITH (...);
-- 显示给定表的建表语句DDL
SHOW CREATE TABLE my_table;
DESCRIBE my_table;
DESC my_table;

TRUNCATE my_table;

-- 显示给定表的分区信息
SHOW PARTITIONS my_table;
SHOW PARTITIONS my_table PARTITION (key=value1,key2=value2);
SHOW PARTITIONS catalog1.database1.my_table;

--


-- 分析 给定表、分区、列的统计信息，并保存到 catalog 中
ANALYZE TABLE my_table COMPUTE STATISTICS;
ANALYZE TABLE my_table COMPUTE STATISTICS FOR ALL COLUMNS;
ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS location;
ANALYZE TABLE my_table PARTITION(sold_year='2022', sold_month='1', sold_day='10') COMPUTE STATISTICS;
ANALYZE TABLE my_table PARTITION(sold_year='2022', sold_month='1', sold_day) COMPUTE STATISTICS;
----

### view

[source,sql]
----
SHOW VIEWS;
CREATE VIEW my_view AS SELECT * FROM my_table;
-- 显示给定视图的建表语句DDL
SHOW CREATE VIEW my_view;
DESCRIBE my_view;
DESC my_view;
----

### column

[source,sql]
----
-- 所有给定表的相关列
SHOW COLUMNS FROM xxx_table LIKE '%f%'
----


### select

[source,sql]
----
SELECT `count`, COUNT(word) FROM my_table WHERE id >10;
EXPLAIN PLAN FOR SELECT ...;
EXPLAIN ESTIMATED_COST, CHANGELOG_MODE, PLAN_ADVICE, JSON_EXECUTION_PLAN SELECT ...;
----



### function

[source,sql]
----
SHOW FUNCTIONS;
SHOW USER FUNCTIONS;
CREATE FUNCTION f1 AS ... ;
----

### x

----
SHOW PROCEDURES;
SHOW PROCEDURES FROM cataloga1.database1;
SHOW PROCEDURES NOT LIKE 'xxx%';
----



### job
[source,sql]
----
SHOW JOBS;
----

### USER

[source,sql]
----
----



### module
* link:https://github.com/apache/flink-connector-hive[apache/flink-connector-hive]
** link:https://github.com/apache/flink-connector-hive/blob/main/flink-connector-hive/src/main/java/org/apache/flink/table/module/hive/HiveModule.java[org.apache.flink.table.module.hive.HiveModule]
* link:[org.apache.hadoop.hive.ql.udf.generic.GenericUDTF]



## docker 运行

* link:https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/[Docker Setup]


[source,shell]
----
# docker pull docker.io/flink:1.18.1-scala_2.12-java11
FLINK_TAG=1.18.1-scala_2.12-java11
FLINK_PROPERTIES="jobmanager.rpc.address: jobmanager"
docker network create flink-network

docker run \
    --rm \
    --env FLINK_PROPERTIES="${FLINK_PROPERTIES}" \
    --name=jobmanager \
    --network flink-network \
    docker.io/flink:${FLINK_TAG} standalone-job \
    --job-classname com.job.ClassName

docker run \
    --env FLINK_PROPERTIES="${FLINK_PROPERTIES}" \
    docker.io/flink:${FLINK_TAG} taskmanager
----

docker-compose.yaml
[source,yaml]
----
version: "2.2"
services:
  jobmanager:
    image: flink:1.18.1-scala_2.12-java11
    ports:
      - "8081:8081"
    command: standalone-job
    #command: standalone-job --job-classname com.job.ClassName [--job-id <job id>] [--jars /path/to/artifact1,/path/to/artifact2] [--fromSavepoint /path/to/savepoint] [--allowNonRestoredState] [job arguments]
    #volumes:
    #  - /host/path/to/job/artifacts:/opt/flink/usrlib
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        parallelism.default: 2

  taskmanager:
    image: flink:1.18.1-scala_2.12-java11
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    #volumes:
    #  - /host/path/to/job/artifacts:/opt/flink/usrlib
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2
----

启动
[source,shell]
----
docker-compose up
----


浏览器访问web 控制台 : link:http://localhost:8081/#/overview[http://localhost:8081/#/overview]


# sql

* link:https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/functions/systemfunctions/#collection-functions[System (Built-in) Functions],  重点关注下 【Collection Functions】、【Value Construction Functions】

[source,sql]
----
-- #################### array
CARDINALITY(array)      -- 返回 array 的长度
array[1]                -- 返回 array 第1个元素的值。注意：下标索引从1开始。
ELEMENT(array)          -- 返回 array 中唯一的一个元素的值，需要 array 长度为1

-- #################### map
CARDINALITY(map)        -- 返回 map 的长度(有多少个entry)
map["key1"]             -- 返回 map 中给定key的值

-- #################### row
("value1", "value2")                        -- 构建一个 ROW 对象
ARRAY["value1", "value2"]                   -- 构建一个 ARRAY 对象
MAP  ["key1", "value1", "key2", "value2"]   -- 构建一个 MAP 对象
----


## 类型转换



[source,sql]
----
NOW()              -- 返回 TIMESTAMP_LTZ
CURRENT_TIMESTAMP  -- 同 NOW()
CURRENT_DATE       -- 返回 DATE 类型

CAST( value AS type)  --
DATE => TIMESTAMP WITH TIME ZONE


DATE_FORMAT(xxxTimeStamp, 'yyyyMMdd')
----


## java 类

[source,plain]
----
org.apache.flink.api.common.typeinfo.TypeInformation        # Stream API 中使用
    org.apache.flink.api.common.typeutils.CompositeType
        org.apache.flink.api.java.typeutils.PojoTypeInfo
    org.apache.flink.api.java.typeutils.MapTypeInfo
org.apache.flink.api.common.typeinfo.Types#POJO

org.apache.flink.table.types.DataType                       # Table API 中使用
    org.apache.flink.table.api.Schema
    org.apache.flink.table.catalog.ResolvedSchema
    org.apache.flink.table.types.logical.LogicalType
        #getDefaultConversion
        org.apache.flink.table.types.logical.ZonedTimestampType
        org.apache.flink.table.types.logical.TimestampType
org.apache.flink.table.api.DataTypes
org.apache.flink.table.runtime.typeutils.InternalTypeInfo
org.apache.flink.table.catalog.DataTypeFactory
org.apache.flink.api.java.typeutils.runtime.PojoSerializer
org.apache.flink.table.typeutils.FieldInfoUtils

org.apache.flink.table.types.logical.LogicalType


org.apache.flink.table.functions.BuiltInFunctionDefinitions # 系统内置函数
----


## JDK 兼容性

.flink与JDK版本兼容性
[,cols="1,1,1"]
|===
|flink version | jdk version | memo |

|1.10.0        | 11 supported and recommended | |
|1.15.0        | 11 recommended, 8 deprecated | |
|1.18.0        | 17 Experimental supported    | |
|===
